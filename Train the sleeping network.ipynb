{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchbearer\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torchbearer.callbacks import LiveLossPlot\n",
    "from itertools import product\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert to tensor\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # flatten into vector\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkControl(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NetworkControl, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, 0.2)        \n",
    "        out = self.fc2(out)        \n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, 0.2)        \n",
    "        out = self.fc3(out)\n",
    "        if not self.training:\n",
    "            out = F.softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize data for sleeping network \n",
    "trainset = MNIST(\".\", train=True, download=True, transform=transform)\n",
    "testset = MNIST(\".\", train=False, download=True, transform=transform)\n",
    "#split the data\n",
    "trainset.data = trainset.data[0:27105]\n",
    "trainset.targets = trainset.targets[0:27105]\n",
    "# # print(trainset.targets[0:11905])\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea507e23e3714813b8baa2dc3f66972d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='0/1(e)', max=79.0, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'test_loss': 1.565501093864441, 'test_acc': 0.9343999624252319}\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('save_ann.pkl')\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# live_loss_plot = LiveLossPlot()\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "# trial = torchbearer.Trial(model, optimiser, loss_function, callbacks=[live_loss_plot], metrics=['loss', 'accuracy']).to(device)\n",
    "trial = torchbearer.Trial(model, optimiser, loss_function,metrics=['loss', 'accuracy']).to(device)\n",
    "trial.with_generators(trainloader, test_generator=testloader)\n",
    "results = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep(model, inputs, scales):\n",
    "    changenum = 0\n",
    "    #The parameters like alpha,beta,etc,you can change to find better performance\n",
    "    num_features = inputs.shape[0]\n",
    "    Ts = inputs.shape[1]\n",
    "    dt = 0.01\n",
    "    InputRate = 40\n",
    "    sleepDur = inputs.shape[1]\n",
    "    dec = 0.03\n",
    "    sleep_inc = 0.0065\n",
    "    sleep_dec = 0.0069 \n",
    "    sleep_beta = [15.579, 0.35, 16.52]  \n",
    "#     25.516308 0.344342 13.197703\n",
    "    previous_factor = 1\n",
    "    alpha = 0.98 \n",
    "    updateswitch = 0\n",
    "    \n",
    "    norm_constants = torch.zeros(3,1).cuda(device)\n",
    "    \n",
    "    #initalize spiking network\n",
    "    InputLayer_spikes = torch.zeros(784,1).cuda(device)\n",
    "    InputLayer_mem = torch.zeros(784,1).cuda(device)\n",
    "    InputLayer_refrac_end = torch.zeros(784,1).cuda(device)\n",
    "    InputLayer_sum_spikes = torch.zeros(784,1).cuda(device)\n",
    "    \n",
    "    fc1_spikes = torch.zeros(1200,1).cuda(device)\n",
    "    fc1_mem = torch.zeros(1200,1).cuda(device)\n",
    "    fc1_refrac_end = torch.zeros(1200,1).cuda(device)\n",
    "    fc1_sum_spikes = torch.zeros(1200,1).cuda(device)\n",
    "    \n",
    "    fc2_spikes = torch.zeros(1200,1).cuda(device)\n",
    "    fc2_mem = torch.zeros(1200,1).cuda(device)\n",
    "    fc2_refrac_end = torch.zeros(1200,1).cuda(device)\n",
    "    fc2_sum_spikes = torch.zeros(1200,1).cuda(device)\n",
    "\n",
    "    fc3_spikes = torch.zeros(10,1).cuda(device)\n",
    "    fc3_mem = torch.zeros(10,1).cuda(device)\n",
    "    fc3r_refrac_end = torch.zeros(10,1).cuda(device)\n",
    "    fc3_sum_spikes = torch.zeros(10,1).cuda(device)\n",
    "    \n",
    "    ListSpikes = [InputLayer_spikes, fc1_spikes, fc2_spikes, fc3_spikes]\n",
    "    ListMem = [InputLayer_mem, fc1_mem, fc2_mem, fc3_mem]\n",
    "    ListRefrac_end= [InputLayer_refrac_end, fc1_refrac_end, fc2_refrac_end, fc3r_refrac_end]\n",
    "    ListSum_spikes = [InputLayer_sum_spikes, fc1_sum_spikes, fc2_sum_spikes, fc3_sum_spikes]\n",
    "    weight = [torch.t(torch.t(model.fc1.weight)), torch.t(torch.t(model.fc2.weight)), torch.t(torch.t(model.fc3.weight))]\n",
    "#     threshold = [0.03618, 0.02336, 0.03638]  \n",
    "    \n",
    "    \n",
    "    \n",
    "    #get normalization norm_constants\n",
    "    for i in range(3):\n",
    "        weight_max = torch.max(weight[i]);\n",
    "        if   weight_max<0:\n",
    "            weight_max = 0\n",
    "        applied_factor = weight_max / previous_factor\n",
    "        norm_constants[i] = 1 / applied_factor\n",
    "        previous_factor = applied_factor\n",
    "            \n",
    "    sleep_alpha = norm_constants*alpha\n",
    "    inp_image = torch.zeros(num_features,1).cuda(device) \n",
    "    \n",
    "    \n",
    "    for i in range(Ts):\n",
    "#          Create poisson distributed spikes from the input images\n",
    "        spike_snapshot = torch.rand(num_features,1) * (1/(dt*InputRate))/2\n",
    "        for idx in range(num_features):\n",
    "            if spike_snapshot[idx] <= inputs[idx,i]:\n",
    "                inp_image[idx] = 1\n",
    "\n",
    "\n",
    "        ListSpikes[0] = inp_image\n",
    "        ListSum_spikes[0] = ListSum_spikes[0]  + inp_image\n",
    "\n",
    "        for i in range(1,4):\n",
    "\n",
    "#             Get input impulse from incoming spike(lose some parameters)\n",
    "            impulse = sleep_alpha[i-1] * torch.t(ListSpikes[i-1]) @ torch.t(weight[i-1])\n",
    "#             Add input to membrane potential\n",
    "\n",
    "            decMem = dec * ListMem[i]\n",
    "            decMem =decMem.cuda(device)\n",
    "            ListMem[i] = decMem + torch.t(impulse)\n",
    "            if i == 3:\n",
    "                ListMem[i] = ListMem[i]\n",
    "\n",
    "\n",
    "            if(i == 1):\n",
    "                print(\"the max of first layer\", torch.max(ListMem[i]))\n",
    "            if(i == 2):\n",
    "                print(\"the max of second layer\", torch.max(ListMem[i]))\n",
    "            if(i == 3):\n",
    "                print(\"the max of third layer\", torch.max(ListMem[i]))\n",
    "\n",
    "#           Check for spiking     \n",
    "            for j in range (ListMem[i].shape[0]):\n",
    "                if ListMem[i][j] >= sleep_beta[i-1]:\n",
    "                    ListSpikes[i][j] = 1\n",
    "                else:\n",
    "                    ListSpikes[i][j] = 0\n",
    "\n",
    "            post_1 = []  \n",
    "            pre_1 = []\n",
    "            pre_0 = []\n",
    "#           STDP\n",
    "            for j in range(ListSpikes[i].shape[0]):\n",
    "                if ListSpikes[i][j] == 1:\n",
    "                    print(j)\n",
    "                    post_1.append(j)                  \n",
    "            for j in range(ListSpikes[i-1].shape[0]):\n",
    "                if ListSpikes[i-1][j] == 1:\n",
    "                    pre_1.append(j)                \n",
    "                else:\n",
    "                    pre_0.append(j)\n",
    "\n",
    "#             print(\"1\",len(post_1))\n",
    "#             print(\"2\",len(pre_1))\n",
    "#             print(list(product(post_1,pre_1)))\n",
    "#             print(len(pre_0))\n",
    "\n",
    "            for idex in list(product(post_1,pre_1)):\n",
    "                updateswitch = 1                         \n",
    "                weight[i-1][idex] = weight[i-1][idex]  - sleep_dec * torch.sigmoid(weight[i-1][idex] )\n",
    "            for idex in list(product(post_1,pre_0)):\n",
    "                updateswitch = 1                                                                      \n",
    "                weight[i-1][idex] = weight[i-1][idex]  + sleep_inc * torch.sigmoid(weight[i-1][idex] )\n",
    "\n",
    "            print(\"The status of switch is \",updateswitch)\n",
    "            if updateswitch == 1:\n",
    "                if i == 1:\n",
    "                    changenum +=1\n",
    "                    fc1_new_weight=torch.nn.Parameter(weight[0])\n",
    "                    model.fc1.weight = fc1_new_weight\n",
    "                if i == 2:\n",
    "                    changenum +=1\n",
    "                    fc2_new_weight=torch.nn.Parameter(weight[1])\n",
    "                    model.fc2.weight = fc2_new_weight\n",
    "                if i == 3:\n",
    "                    changenum +=1\n",
    "                    fc3_new_weight=torch.nn.Parameter(weight[2])\n",
    "                    model.fc3.weight = fc3_new_weight\n",
    "\n",
    "            updateswitch = 0  #reset switch                  \n",
    "\n",
    "            #reset\n",
    "            for j in range(ListSpikes[i].shape[0]):\n",
    "                if ListSpikes[i][j] == 1:\n",
    "                    ListMem[i][j] = 0\n",
    "            \n",
    "            #To save the memory space, we just see what happened after the weight changed 20 times\n",
    "            if changenum >= 20:\n",
    "                print(\"changenum is \",changenum)\n",
    "                sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the max of first layer tensor(8.7730, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(11.5286, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(16.7887, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "1156\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.8015, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "541\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0528, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.5867, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "634\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0587, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(16.2132, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "419\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0509, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.8943, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "541\n",
      "1056\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0953, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.6375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "834\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0493, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(18.3982, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "154\n",
      "342\n",
      "703\n",
      "777\n",
      "842\n",
      "949\n",
      "1156\n",
      "1159\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.2734, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.3293, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0.0082, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(16.9788, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "861\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0714, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.3059, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0.0021, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(16.5263, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "32\n",
      "103\n",
      "444\n",
      "665\n",
      "1019\n",
      "1143\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.1914, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.4601, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0.0057, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.4612, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0.0002, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.8435, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "143\n",
      "1152\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0977, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(16.4650, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "618\n",
      "981\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0937, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(17.3051, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "120\n",
      "525\n",
      "634\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.1203, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "1186\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0510, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(16.6551, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "541\n",
      "626\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.1022, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.0618, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0.0031, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.5394, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(9.1954e-05, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.6334, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "790\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0532, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.0357, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0.0016, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.0358, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(4.7921e-05, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.8112, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "4\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0535, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.0107, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0.0016, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(18.6478, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "248\n",
      "279\n",
      "442\n",
      "893\n",
      "947\n",
      "1156\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.2038, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(16.6778, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "266\n",
      "419\n",
      "457\n",
      "590\n",
      "1196\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.1678, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.4860, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0.0050, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.4626, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0.0002, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.4619, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(4.5312e-06, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.7825, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "320\n",
      "The status of switch is  1\n",
      "the max of second layer tensor(0.0488, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.3749, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(0.0015, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.3809, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(4.3904e-05, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.3811, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(1.3171e-06, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.3811, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of second layer tensor(3.9514e-08, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of third layer tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "The status of switch is  0\n",
      "the max of first layer tensor(15.6302, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "134\n",
      "The status of switch is  1\n",
      "changenum is  20\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "for data in trainloader:\n",
    "        # get the inputs\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.cuda(device)\n",
    "    labels = labels.cuda(device)\n",
    "    sleep(model, torch.t(inputs),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93651e4bd38d434cb30d34bd3bbd7d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='0/1(e)', max=79.0, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'test_loss': 1.573760986328125, 'test_acc': 0.9289000034332275}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type NetworkControl. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "# live_loss_plot = LiveLossPlot()\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "trial = torchbearer.Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy']).to(device)\n",
    "trial.with_generators(trainloader, test_generator=testloader)\n",
    "results = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
    "print(results)\n",
    "torch.save(model,'save_sleep.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
